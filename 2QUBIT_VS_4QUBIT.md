# 2-Qubit vs 4-Qubit: Understanding the Difference

## Why the Confusion?

The original implementation used **2 qubits**, but Schuld & Petruccione's book specifies **4 qubits**. Here's why:

---

## ğŸ”´ 2-Qubit Implementation (Simplified Version)

### Structure
- **q0**: Class qubit (superposition of |0âŸ© + |1âŸ©)
- **q1**: Compressed data encoding (both features squeezed into rotation angles)

### Amplitude Vector (4 elements)
```
|ÏˆâŸ© = [Î±â‚€â‚€, Î±â‚€â‚, Î±â‚â‚€, Î±â‚â‚]
```

### Limitations
1. **Compressed encoding**: Both 2D features mapped to a single qubit via rotation angles
2. **No explicit label qubit**: Label encoded implicitly in superposition
3. **Oversimplified**: Doesn't match book's actual circuit structure

### Circuit
```
q0: â”€Hâ”€[Ïˆ_prep]â”€Hâ”€M
q1: â”€â”€â”€[Ïˆ_prep]â”€â”€â”€M
```

---

## âœ… 4-Qubit Implementation (Book's Exact Specification)

### Structure
- **q0**: Ancilla qubit (for Hadamard interference)
- **q1**: Feature bit 0 (ticket price amplitude)
- **q2**: Feature bit 1 (cabin number amplitude)
- **q3**: Label qubit (survival outcome)

### Amplitude Vector (16 elements)
```
|ÏˆâŸ© = [Î±â‚€â‚€â‚€â‚€, Î±â‚€â‚€â‚€â‚, Î±â‚€â‚€â‚â‚€, ..., Î±â‚â‚â‚â‚]
      â†‘ q0   â†‘ q1   â†‘ q2   â†‘ q3
```

### Key Features
1. **Explicit amplitude encoding**: Each feature gets its own qubit(s)
2. **Dedicated label qubit**: Clear separation of data and labels
3. **Test point duplication**: P3 appears twice (with label=0 and label=1)
4. **Matches book exactly**: All specifications align with Chapter 1.2

### Circuit
```
q0: â”€Initializeâ”€Hâ”€Mâ”€
q1: â”€Initializeâ”€â”€â”€â”€â”€
q2: â”€Initializeâ”€â”€â”€â”€â”€
q3: â”€Initializeâ”€â”€â”€Mâ”€
```

---

## ğŸ“Š Comparison Table

| Feature | 2-Qubit (Simplified) | 4-Qubit (Book Exact) |
|---------|---------------------|----------------------|
| **Amplitude slots** | 4 (2Â²) | 16 (2â´) |
| **Feature encoding** | Compressed (rotation) | Explicit (amplitude) |
| **Label qubit** | No (implicit) | Yes (q3) |
| **Ancilla qubit** | q0 (dual purpose) | q0 (dedicated) |
| **Test point** | Once | Twice (both labels) |
| **Normalization** | 1/âˆš2 | 1/âˆš4 |
| **Book fidelity** | âŒ Approximate | âœ… Exact |
| **Complexity** | Lower | Higher |
| **Pedagogical value** | Introduction | Full algorithm |

---

## ğŸ¯ Why 4 Qubits for 2D Data?

### Logical Breakdown

**For 2D data [xâ‚€, xâ‚], we need to encode:**

1. **Training data**: P1=[xâ‚€, xâ‚] with label=1, P2=[xâ‚€, xâ‚] with label=0
2. **Test data**: P3=[xâ‚€, xâ‚] with label=? (unknown)

**Amplitude encoding requirements:**

- P1 needs 2 slots (xâ‚€, xâ‚) + label=1
- P2 needs 2 slots (xâ‚€, xâ‚) + label=0
- P3 needs 4 slots (xâ‚€, xâ‚ twice: once with label=0, once with label=1)

**Total: 8 data slots + labels â†’ requires 4 qubits (2â´ = 16 slots)**

### Why Duplicate P3?

The quantum interference (Hadamard on q0) computes:
```
H|0âŸ©|P3,label=0âŸ© = (|0âŸ© + |1âŸ©)|P3,label=0âŸ©
H|1âŸ©|P3,label=1âŸ© = (|0âŸ© - |1âŸ©)|P3,label=1âŸ©
```

Post-selecting on |0âŸ© creates the sum: |P3,label=0âŸ© + |P3,label=1âŸ©

This is the **key quantum trick** that enables computing similarities to both training prototypes simultaneously!

---

## ğŸ§® Mathematical Proof: Why 4 Qubits?

### Amplitude Vector Structure

**Goal:** Encode 3 passengers Ã— 2 features Ã— 2 label options

**Required amplitude slots:**
```
P1 with label=1: 2 amplitudes (xâ‚€, xâ‚)
P2 with label=0: 2 amplitudes (xâ‚€, xâ‚)
P3 with label=0: 2 amplitudes (xâ‚€, xâ‚)  â† First copy
P3 with label=1: 2 amplitudes (xâ‚€, xâ‚)  â† Second copy (for interference)
```

**Total: 8 non-zero amplitudes**

**Minimum qubits needed:**
- 3 qubits â†’ 2Â³ = 8 slots âŒ (not enough for label separation)
- 4 qubits â†’ 2â´ = 16 slots âœ… (with room for label qubit)

### Qubit Assignment Logic

```
q0 (ancilla): Controls which "block" of training data
  |0âŸ© â†’ Training block (P1, P2)
  |1âŸ© â†’ Test block (P3, P3)

q1-q2 (features): Encode 2D data via amplitude
  |00âŸ© â†’ component 0 of feature 0
  |01âŸ© â†’ component 1 of feature 0
  |10âŸ© â†’ component 0 of feature 1
  |11âŸ© â†’ component 1 of feature 1

q3 (label): Survival outcome
  |0âŸ© â†’ Did not survive
  |1âŸ© â†’ Survived
```

---

## ğŸ“ˆ When to Use Each?

### Use 2-Qubit Implementation When:
- âœ… Learning quantum ML concepts
- âœ… Quick prototyping
- âœ… Resource-constrained environments
- âœ… You need a "quantum-inspired" approach

### Use 4-Qubit Implementation When:
- âœ… Reproducing book's exact results
- âœ… Academic paper/thesis work
- âœ… Teaching Schuld & Petruccione Chapter 1.2
- âœ… Understanding true amplitude encoding
- âœ… You need the pedagogically correct version

---

## ğŸ”¬ Key Takeaway

The **2-qubit version is pedagogically incomplete** because it:
1. Doesn't show explicit amplitude encoding
2. Compresses features artificially
3. Lacks the label qubit structure
4. Can't demonstrate the test-point duplication trick

The **4-qubit version is the book's actual algorithm** because it:
1. âœ… Uses explicit amplitude encoding (book's main teaching point)
2. âœ… Has dedicated label qubit (shows data/label separation)
3. âœ… Duplicates test point (demonstrates quantum interference)
4. âœ… Matches all book equations and circuit diagrams

---

## ğŸ’¡ Historical Note

Many early quantum ML tutorials simplified the algorithm to 2 qubits because:
- Easier to explain to beginners
- Smaller state vectors to visualize
- Less memory for classical simulation
- But this **sacrifices the key pedagogical insights**!

Schuld & Petruccione deliberately chose 4 qubits to demonstrate:
1. **Amplitude encoding** with real quantum states
2. **Label qubits** as a core ML concept
3. **Quantum interference** for computing similarities
4. **Post-selection** as a quantum measurement strategy

---

## âœ… Implementation Checklist

**Have you verified your implementation is using the EXACT book specification?**

- [ ] 4 qubits (not 2!)
- [ ] 16-element amplitude vector
- [ ] Normalization factor Î± = 1/âˆš4
- [ ] Test point (P3) appears twice (indices 9,11 and 12,14)
- [ ] Label qubit q3 separated from data qubits q1-q2
- [ ] Ancilla qubit q0 for Hadamard only
- [ ] Post-selection on q0 = 0
- [ ] Classification from q3 measurement
- [ ] Results: p(survive)â‰ˆ0.552, p(die)â‰ˆ0.448

**If any box is unchecked, you're not implementing the book's exact algorithm!**

---

## ğŸ“š Further Reading

1. **Schuld & Petruccione (2018), Chapter 1.2**  
   Pages 12-18: The exact toy example with 4-qubit circuit diagram

2. **Why amplitude encoding?**  
   Chapter 1.1: Data encoding strategies for quantum ML

3. **Post-selection overhead**  
   Chapter 1.2.4: Discussion of practical limitations

4. **Quantum advantage?**  
   Chapter 1.3: Why this algorithm is classically simulable

---

**Bottom Line:** Always use the **4-qubit implementation** for faithful reproduction of Schuld & Petruccione Chapter 1.2!
